{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JobcExTi465J"},"outputs":[],"source":["!pip install datasets py7zr transformers evaluate rouge_score accelerate wandb"]},{"cell_type":"code","source":["from datasets import load_dataset\n","dataset = load_dataset('samsum')"],"metadata":{"id":"wRLD3yVY4-Vb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"Train dataset size: {len(dataset['train'])}\")\n","print(f\"Test dataset size: {len(dataset['test'])}\")"],"metadata":{"id":"NKLXub0d5IqU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from random import randrange        \n","sample = dataset['train'][randrange(len(dataset[\"train\"]))]\n","print(f\"dialogue: \\n{sample['dialogue']}\\n---------------\")\n","print(f\"summary: \\n{sample['summary']}\\n---------------\")"],"metadata":{"id":"jISPGXPr5VDu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, BartForConditionalGeneration, T5ForConditionalGeneration\n","model_str = 'sshleifer/distilbart-cnn-6-6'\n","tokenizer = AutoTokenizer.from_pretrained(model_str)\n","model = BartForConditionalGeneration.from_pretrained(model_str)\n","# model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-small\", device_map=\"auto\")"],"metadata":{"id":"zo5ATRpZ5Yic"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"r9-inQpJ4PcK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import concatenate_datasets\n","\n","tokenized_inputs = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"dialogue\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_source_length = max([len(x) for x in tokenized_inputs[\"input_ids\"]])\n","print(f\"Max source length: {max_source_length}\")\n","tokenized_targets = concatenate_datasets([dataset[\"train\"], dataset[\"test\"]]).map(lambda x: tokenizer(x[\"summary\"], truncation=True), batched=True, remove_columns=[\"dialogue\", \"summary\"])\n","max_target_length = max([len(x) for x in tokenized_targets[\"input_ids\"]])\n","print(f\"Max target length: {max_target_length}\")"],"metadata":{"id":"rYw5jaHU6XrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess_function(sample,padding=\"max_length\"):\n","    inputs = [\"summarize: \" + item for item in sample[\"dialogue\"]]\n","    model_inputs = tokenizer(inputs, max_length=512, padding=padding, truncation=True)\n","\n","    labels = tokenizer(text_target=sample[\"summary\"], max_length=max_target_length, padding=padding, truncation=True)\n","\n","    if padding == \"max_length\":\n","        labels[\"input_ids\"] = [\n","            [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n","        ]\n","\n","    model_inputs[\"labels\"] = labels[\"input_ids\"]\n","    return model_inputs\n","\n","tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=[\"dialogue\", \"summary\", \"id\"])\n","print(f\"Keys of tokenized dataset: {list(tokenized_dataset['train'].features)}\")"],"metadata":{"id":"lllyf_4c6jsV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from google.colab import drive\n","# drive.mount('/content/drive')"],"metadata":{"id":"XPio7OgKFchB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import evaluate\n","import nltk\n","import numpy as np\n","from nltk.tokenize import sent_tokenize\n","nltk.download(\"punkt\")\n","\n","# Metric\n","rouge = evaluate.load(\"rouge\")\n","google_bleu = evaluate.load(\"google_bleu\")\n","\n","# helper function to postprocess text\n","def postprocess_text(preds, labels):\n","    preds = [pred.strip() for pred in preds]\n","    labels = [label.strip() for label in labels]\n","\n","    # rougeLSum expects newline after each sentence\n","    preds = [\"\\n\".join(sent_tokenize(pred)) for pred in preds]\n","    labels = [\"\\n\".join(sent_tokenize(label)) for label in labels]\n","\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","    metric_dict = {}\n","    rogue_score = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    gleu_score = google_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n","    rogue_score = {k: round(v * 100, 4) for k, v in rogue_score.items()}\n","    gleu_score = {k: round(v * 100, 4) for k, v in gleu_score.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    metric_dict[\"gen_len\"] = np.mean(prediction_lens)\n","    for k,v in rogue_score.items():\n","        metric_dict[k] = v\n","    for k,v in gleu_score.items():\n","        metric_dict[k] = v\n","    return metric_dict"],"metadata":{"id":"BdL3nnEZ6q4j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","def save_outputs(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # Replace -100 in the labels as we can't decode them.\n","    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    \n","    # Some simple post-processing\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","    with open(f\"/content/drive/MyDrive/ECE285-Project/preds_{model_str}.json\",'w') as f:\n","        json.dump(decoded_preds,f)\n","\n","    metric_dict = {}\n","    rogue_score = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    gleu_score = google_bleu.compute(predictions=decoded_preds, references=decoded_labels)\n","    rogue_score = {k: round(v * 100, 4) for k, v in rogue_score.items()}\n","    gleu_score = {k: round(v * 100, 4) for k, v in gleu_score.items()}\n","    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n","    metric_dict[\"gen_len\"] = np.mean(prediction_lens)\n","    for k,v in rogue_score.items():\n","        metric_dict[k] = v\n","    for k,v in gleu_score.items():\n","        metric_dict[k] = v\n","    return metric_dict"],"metadata":{"id":"HhLStRv0xhS-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DataCollatorForSeq2Seq\n","\n","# we want to ignore tokenizer pad token in the loss\n","label_pad_token_id = -100\n","# Data collator\n","data_collator = DataCollatorForSeq2Seq(\n","    tokenizer,\n","    model=model,\n","    label_pad_token_id=label_pad_token_id,\n","    pad_to_multiple_of=4\n",")"],"metadata":{"id":"WOkmOeMS63vY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from huggingface_hub import HfFolder\n","from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n","\n","# Hugging Face repository id\n","repository_id = f\"/content/drive/MyDrive/ECE285-Project/{model_str}\"\n","\n","# Define training args\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=repository_id,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    predict_with_generate=True,\n","    fp16=False, # Overflows with fp16\n","    learning_rate=5e-5,\n","    num_train_epochs=5,\n","    # logging & evaluation strategies\n","    logging_dir=f\"{repository_id}/logs\",\n","    logging_strategy=\"steps\",\n","    logging_steps=500,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    save_total_limit=1,\n","    load_best_model_at_end=True,\n","    # metric_for_best_model=\"overall_f1\",\n","    # push to hub parameters\n","    report_to=\"wandb\",\n","    push_to_hub=False,\n","    hub_strategy=\"every_save\",\n","    hub_model_id=repository_id,\n","    hub_token=HfFolder.get_token(),\n",")\n","\n","# Create Trainer instance\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=tokenized_dataset[\"train\"],\n","    eval_dataset=tokenized_dataset[\"test\"],\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"JMA62BhR68lx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer.train()\n","# trainer.evaluate()\n","model_to_save = trainer.model.module if hasattr(trainer.model, 'module') else trainer.model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained('BART_SAMSUM')"],"metadata":{"id":"EUSu9by97l-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# trainer.evaluate()"],"metadata":{"id":"DZn52U284_oG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# # model = T5ForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/ECE285-Project/BART_SAMSUM/checkpoint-921\", device_map=\"auto\")\n","model = BartForConditionalGeneration.from_pretrained(\"/content/drive/MyDrive/ECE285-Project/sshleifer/distilbart-cnn-6-6/checkpoint-3684\",device_map=\"auto\")\n","\n","test_args = Seq2SeqTrainingArguments(\n","    output_dir='BART_SAMSUM',\n","    do_train=False,\n","    do_predict=True,\n","    per_device_eval_batch_size=8,\n","    predict_with_generate=True)\n","\n","\n","trainer = Seq2SeqTrainer(\n","            model=model,\n","            args=training_args,\n","            data_collator=data_collator,\n","            train_dataset=tokenized_dataset[\"train\"],\n","            eval_dataset=tokenized_dataset[\"test\"],\n","            compute_metrics = save_outputs)\n","\n","trainer.evaluate()"],"metadata":{"id":"2LWHcjwa4ob3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# preds_file = '/content/drive/MyDrive/ECE285-Project/preds.json'\n","# with open(preds_file,'r') as f:\n","#     preds = json.load(f)"],"metadata":{"id":"nlsL0rAW426L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# dataset['test'][0]['dialogue']"],"metadata":{"id":"1hW3ZSXnCKk9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n","\n","# model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-xxl\")\n","# tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-xxl\")\n","\n","# conversation = dataset['test'][0]['dialogue']\n","# summary = preds[0]\n","# prompt =f'Given is a conversation {conversation} and its corresponding summary {summary}, please improve upon the summary'\n","\n","# inputs = tokenizer(\"\", return_tensors=\"pt\")\n","# outputs = model.generate(**inputs)\n","# print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"],"metadata":{"id":"eG4uvCiP56eY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"8xOmJ4ThAJJC"},"execution_count":null,"outputs":[]}]}